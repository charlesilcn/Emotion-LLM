import os
import sys
import logging
import argparse
import pandas as pd
from typing import Optional

# é…ç½®æ—¥å¿— - ä¼˜åŒ–ä¸ºè¯¾å ‚å±•ç¤ºé£æ ¼
logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from src.data.data_loader import DataLoader
from src.data.preprocessor import TextPreprocessor, ChineseTextPreprocessor
# å°è¯•å¯¼å…¥LLMSentimentAnalyzerï¼Œå¦‚æœå¤±è´¥åˆ™è®¾ç½®ä¸ºNone
try:
    from src.analysis.llm_sentiment_analyzer import LLMSentimentAnalyzer
    logger.info("æˆåŠŸå¯¼å…¥LLMSentimentAnalyzer")
except ImportError as e:
    logger.warning(f"å¯¼å…¥LLMSentimentAnalyzerå¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨åŸºæœ¬åŠŸèƒ½")
    LLMSentimentAnalyzer = None

# å°è¯•å¯¼å…¥TraditionalSentimentAnalyzerï¼Œå¦‚æœå¤±è´¥åˆ™åˆ›å»ºä¸€ä¸ªç®€å•çš„å¤‡ç”¨
try:
    from src.analysis.traditional_sentiment_analyzer import TraditionalSentimentAnalyzer
    logger.info("æˆåŠŸå¯¼å…¥TraditionalSentimentAnalyzer")
except ImportError as e:
    logger.warning(f"å¯¼å…¥TraditionalSentimentAnalyzerå¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨ç®€å•å¤‡ç”¨åˆ†æå™¨")
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„å¤‡ç”¨æƒ…æ„Ÿåˆ†æå™¨ç±»
    class SimpleSentimentAnalyzer:
        def __init__(self):
            # ç®€å•çš„å…³é”®è¯æƒ…æ„Ÿè¯å…¸
            self.pos_words = set(['å¥½', 'æ£’', 'ä¼˜ç§€', 'å–œæ¬¢', 'å¼€å¿ƒ', 'å¿«ä¹', 'æ»¡æ„', 'èµ', 'æ¨è', 'æ”¯æŒ',
                                'good', 'great', 'excellent', 'like', 'love', 'happy', 'satisfied', 'awesome'])
            self.neg_words = set(['å', 'å·®', 'ç³Ÿç³•', 'è®¨åŒ', 'ç”Ÿæ°”', 'éš¾è¿‡', 'ä¸æ»¡æ„', 'å‘', 'å¤±æœ›', 'åå¯¹',
                                'bad', 'poor', 'terrible', 'hate', 'angry', 'sad', 'disappointed', 'worst'])
        
        def analyze_text(self, text):
            """ç®€å•çš„æ–‡æœ¬æƒ…æ„Ÿåˆ†æ"""
            if not text or not isinstance(text, str):
                return 0.0  # ä¸­æ€§
            
            words = text.lower().split()
            pos_count = sum(1 for word in words if word in self.pos_words)
            neg_count = sum(1 for word in words if word in self.neg_words)
            
            # è®¡ç®—æƒ…æ„Ÿå¾—åˆ† (-1 åˆ° 1)
            if pos_count + neg_count == 0:
                return 0.0  # æ²¡æœ‰æƒ…æ„Ÿè¯
            
            score = (pos_count - neg_count) / (pos_count + neg_count)
            return score
        
        def analyze_dataframe(self, df, text_column='content'):
            """åˆ†ææ•°æ®æ¡†ä¸­çš„æ–‡æœ¬"""
            df = df.copy()
            # ä½¿ç”¨ç®€å•çš„æƒ…æ„Ÿåˆ†æ
            df['sentiment_score'] = df[text_column].apply(self.analyze_text)
            # åŸºäºå¾—åˆ†åˆ†ç±»
            df['sentiment'] = df['sentiment_score'].apply(
                lambda score: 'positive' if score > 0 else ('negative' if score < 0 else 'neutral')
            )
            return df
    
    TraditionalSentimentAnalyzer = SimpleSentimentAnalyzer
# å°è¯•å¯¼å…¥SentimentVisualizerï¼Œå¦‚æœå¤±è´¥åˆ™åˆ›å»ºä¸€ä¸ªç®€å•çš„å¤‡ç”¨
try:
    from src.visualization.visualizer import SentimentVisualizer
    logger.info("æˆåŠŸå¯¼å…¥SentimentVisualizer")
except ImportError as e:
    logger.warning(f"å¯¼å…¥SentimentVisualizerå¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨ç®€å•å¤‡ç”¨å¯è§†åŒ–")
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„å¤‡ç”¨å¯è§†åŒ–ç±»
    class SimpleVisualizer:
        def __init__(self):
            pass
        
        def plot_sentiment_distribution(self, df, sentiment_column='sentiment'):
            """ç®€å•æ˜¾ç¤ºæƒ…æ„Ÿåˆ†å¸ƒç»Ÿè®¡"""
            logger.info("\nğŸ“Š æƒ…æ„Ÿåˆ†å¸ƒç»Ÿè®¡:")
            sentiment_counts = df[sentiment_column].value_counts()
            for sentiment, count in sentiment_counts.items():
                logger.info(f"  {sentiment}: {count} ({count/len(df)*100:.1f}%)")
            return None
        
        def plot_sentiment_by_category(self, df, category_column, sentiment_column='sentiment'):
            """ç®€å•æ˜¾ç¤ºæŒ‰ç±»åˆ«çš„æƒ…æ„Ÿåˆ†å¸ƒ"""
            if category_column in df.columns:
                logger.info(f"\nğŸ“Š æŒ‰{category_column}çš„æƒ…æ„Ÿåˆ†å¸ƒ:")
                grouped = df.groupby([category_column, sentiment_column]).size().unstack(fill_value=0)
                for category in grouped.index:
                    logger.info(f"  {category}:")
                    for sentiment in grouped.columns:
                        count = grouped.loc[category, sentiment]
                        total = grouped.loc[category].sum()
                        if total > 0:
                            logger.info(f"    {sentiment}: {count} ({count/total*100:.1f}%)")
            return None
        
        def plot_sentiment_time_series(self, df, date_column, sentiment_column='sentiment'):
            """ç®€å•æ˜¾ç¤ºæ—¶é—´åºåˆ—æƒ…æ„Ÿè¶‹åŠ¿"""
            if date_column in df.columns:
                logger.info(f"\nğŸ“Š æƒ…æ„Ÿæ—¶é—´è¶‹åŠ¿:")
                # è¿™é‡Œåªæ˜¯ç®€å•çš„æ—¥æœŸè®¡æ•°
                df['date'] = pd.to_datetime(df[date_column]).dt.date
                daily_counts = df.groupby(['date', sentiment_column]).size().unstack(fill_value=0)
                for date in sorted(daily_counts.index):
                    if date in daily_counts.index:
                        logger.info(f"  {date}:")
                        for sentiment in daily_counts.columns:
                            count = daily_counts.loc[date, sentiment]
                            logger.info(f"    {sentiment}: {count}")
            return None
        
        def save_figures(self, figures, output_dir='./output'):
            """ç®€å•ä¿å­˜æç¤º"""
            logger.info(f"æç¤º: ç”±äºç¼ºå°‘å¯è§†åŒ–åº“ï¼Œæ— æ³•ä¿å­˜å›¾è¡¨åˆ° {output_dir}")
            return []
    
    SentimentVisualizer = SimpleVisualizer
from config import OPENAI_API_KEY, PROCESSED_DATA_DIR

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='ç¤¾äº¤åª’ä½“æ•°æ®é«˜çº§æƒ…æ„Ÿåˆ†æå·¥å…· - è¯¾å ‚æ¼”ç¤ºç‰ˆ')
    
    # æ•°æ®å‚æ•°
    parser.add_argument('--data', type=str, help='è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--text-column', type=str, default='text', help='æ–‡æœ¬åˆ—å')
    parser.add_argument('--output', type=str, default='analysis_results.csv', help='è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„')
    
    # å¤„ç†å‚æ•°
    parser.add_argument('--language', type=str, default='chinese', choices=['english', 'chinese'], help='æ–‡æœ¬è¯­è¨€')
    parser.add_argument('--skip-processing', action='store_true', help='è·³è¿‡æ•°æ®é¢„å¤„ç†')
    
    # åˆ†æå‚æ•° - æ·»åŠ model_typeé€‰é¡¹ï¼Œé»˜è®¤ä½¿ç”¨Hugging Faceæ¨¡å‹
    parser.add_argument('--model-type', type=str, default='huggingface', choices=['openai', 'huggingface'], 
                      help='æ¨¡å‹ç±»å‹ (é»˜è®¤: huggingfaceï¼Œä½¿ç”¨å…è´¹æ¨¡å‹)')
    parser.add_argument('--model', type=str, default='distilbert-base-uncased-finetuned-sst-2-english', 
                      help='æ¨¡å‹åç§° (å¯¹äºHugging Faceï¼Œé»˜è®¤ä½¿ç”¨å…è´¹æƒ…æ„Ÿåˆ†ææ¨¡å‹)')
    parser.add_argument('--use-traditional', action='store_true', help='åŒæ—¶ä½¿ç”¨ä¼ ç»Ÿæƒ…æ„Ÿåˆ†ææ–¹æ³•')
    parser.add_argument('--batch-size', type=int, default=100, help='æ‰¹å¤„ç†å¤§å°')
    
    # å¯è§†åŒ–å‚æ•°
    parser.add_argument('--visualize', action='store_true', help='ç”Ÿæˆå¯è§†åŒ–ç»“æœ')
    parser.add_argument('--save-visualizations', action='store_true', help='ä¿å­˜å¯è§†åŒ–ç»“æœ')
    
    # ç¤ºä¾‹æ•°æ®
    parser.add_argument('--use-sample', action='store_true', help='ä½¿ç”¨ç¤ºä¾‹æ•°æ®')
    
    return parser.parse_args()

def load_sample_data() -> pd.DataFrame:
    """åŠ è½½ç¤ºä¾‹æ•°æ®ï¼ˆå¦‚æœæ²¡æœ‰çœŸå®æ•°æ®ï¼‰"""
    logger.info("ç”Ÿæˆç¤ºä¾‹æ•°æ®...")
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    sample_data = [
        {"text": "æˆ‘éå¸¸å–œæ¬¢è¿™ä¸ªäº§å“ï¼Œè´¨é‡å¾ˆå¥½ï¼Œä»·æ ¼ä¹Ÿå¾ˆåˆç†ï¼", "source": "å¾®åš", "date": "2023-06-15"},
        {"text": "æœåŠ¡æ€åº¦å¾ˆå·®ï¼Œç­‰äº†å¾ˆä¹…éƒ½æ²¡äººç†æˆ‘ï¼Œä¸ä¼šå†æ¥äº†ã€‚", "source": "å¾®ä¿¡", "date": "2023-06-16"},
        {"text": "äº§å“ä¸€èˆ¬èˆ¬ï¼Œæ²¡æœ‰ç‰¹åˆ«æƒŠè‰³çš„åœ°æ–¹ï¼Œä½†ä¹Ÿæ²¡æœ‰æ˜æ˜¾ç¼ºç‚¹ã€‚", "source": "å°çº¢ä¹¦", "date": "2023-06-17"},
        {"text": "ç‰©æµé€Ÿåº¦å¤ªå¿«äº†ï¼Œæ˜¨å¤©æ‰ä¸‹å•ä»Šå¤©å°±æ”¶åˆ°äº†ï¼ŒåŒ…è£…ä¹Ÿå¾ˆç²¾ç¾ï¼", "source": "æ·˜å®", "date": "2023-06-18"},
        {"text": "å®Œå…¨ä¸ç¬¦åˆæè¿°ï¼Œå®ç‰©å’Œå›¾ç‰‡å·®è·å¾ˆå¤§ï¼Œéå¸¸å¤±æœ›ã€‚", "source": "äº¬ä¸œ", "date": "2023-06-19"},
        {"text": "æ€§ä»·æ¯”å¾ˆé«˜ï¼Œè¶…å‡ºé¢„æœŸï¼Œæ¨èç»™å¤§å®¶ï¼", "source": "æŠ–éŸ³", "date": "2023-06-20"},
        {"text": "å®¢æœå¾ˆè€å¿ƒï¼Œè§£å†³é—®é¢˜å¾ˆåŠæ—¶ï¼Œç»™ä¸ªèµï¼", "source": "æ‹¼å¤šå¤š", "date": "2023-06-21"},
        {"text": "è´¨é‡é—®é¢˜å¾ˆä¸¥é‡ï¼Œç”¨äº†ä¸€å¤©å°±åäº†ï¼Œå”®åæœåŠ¡ä¹Ÿä¸å¥½ã€‚", "source": "å¤©çŒ«", "date": "2023-06-22"},
        {"text": "æ•´ä½“è¿˜ä¸é”™ï¼Œå°±æ˜¯ç‰©æµæœ‰ç‚¹æ…¢ï¼Œå…¶ä»–éƒ½å¾ˆæ»¡æ„ã€‚", "source": "é—²é±¼", "date": "2023-06-23"},
        {"text": "è¿™æ˜¯æˆ‘ç”¨è¿‡çš„æœ€å·®çš„äº§å“ï¼Œæ²¡æœ‰ä¹‹ä¸€ï¼Œå¼ºçƒˆä¸æ¨èï¼", "source": "ç¾å›¢", "date": "2023-06-24"},
    ]
    
    df = pd.DataFrame(sample_data)
    
    # ä¿å­˜ç¤ºä¾‹æ•°æ®
    sample_path = os.path.join(PROCESSED_DATA_DIR, 'sample_data.csv')
    df.to_csv(sample_path, index=False, encoding='utf-8')
    logger.info(f"ç¤ºä¾‹æ•°æ®å·²ä¿å­˜åˆ°: {sample_path}")
    
    return df

def preprocess_data(df: pd.DataFrame, text_column: str, language: str = 'chinese') -> pd.DataFrame:
    """é¢„å¤„ç†æ•°æ®"""
    logger.info(f"å¼€å§‹é¢„å¤„ç†æ•°æ®ï¼Œè¯­è¨€: {language}")
    
    # æ ¹æ®è¯­è¨€é€‰æ‹©é¢„å¤„ç†å™¨
    if language == 'chinese':
        preprocessor = ChineseTextPreprocessor()
    else:
        preprocessor = TextPreprocessor(language=language)
    
    # å¤„ç†æ–‡æœ¬
    processed_df = preprocessor.process_dataframe(
        df, 
        text_column,
        remove_urls=True,
        remove_usernames=True,
        remove_hashtags=False,  # ä¿ç•™è¯é¢˜æ ‡ç­¾å¯èƒ½æœ‰ç”¨
        remove_emojis=True,
        lowercase=(language != 'chinese'),  # ä¸­æ–‡ä¸éœ€è¦å°å†™
        remove_punct=True,
        remove_stop=True,
        lemmatize_text=(language != 'chinese')  # ä¸­æ–‡ä¸éœ€è¦è¯å½¢è¿˜åŸ
    )
    
    logger.info(f"æ•°æ®é¢„å¤„ç†å®Œæˆï¼Œä¿ç•™äº† {len(processed_df)} æ¡æœ‰æ•ˆæ•°æ®")
    return processed_df

def analyze_sentiment(df: pd.DataFrame, text_column: str, model_name: str, 
                     model_type: str = 'huggingface', use_traditional: bool = False, 
                     batch_size: int = 100) -> pd.DataFrame:
    """æ‰§è¡Œæƒ…æ„Ÿåˆ†æ"""
    logger.info(f"ğŸ” å¼€å§‹æƒ…æ„Ÿåˆ†æï¼Œä½¿ç”¨æ¨¡å‹ç±»å‹: {model_type}ï¼Œæ¨¡å‹: {model_name}")
    
    # åˆå§‹åŒ–åˆ†æç»“æœDataFrame
    analyzed_df = df.copy()
    
    # ä½¿ç”¨LLMè¿›è¡Œæƒ…æ„Ÿåˆ†æ - ä¼˜å…ˆä½¿ç”¨Hugging Faceå…è´¹æ¨¡å‹
    try:
        # æ£€æŸ¥LLMSentimentAnalyzeræ˜¯å¦å¯ç”¨
        if LLMSentimentAnalyzer is not None:
            try:
                # åˆ›å»ºLLMæƒ…æ„Ÿåˆ†æå™¨ï¼Œä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹ç±»å‹å’Œåç§°
                llm_analyzer = LLMSentimentAnalyzer(
                    model_name=model_name, 
                    model_type=model_type,
                    show_progress=True  # æ˜¾ç¤ºè¿›åº¦æ¡ï¼Œé€‚åˆè¯¾å ‚å±•ç¤º
                )
                
                # æ‰§è¡Œåˆ†æ
                logger.info(f"ğŸ“Š æ­£åœ¨ä½¿ç”¨{model_type.upper()}æ¨¡å‹åˆ†æ {len(df)} æ¡æ–‡æœ¬...")
                analyzed_df = llm_analyzer.analyze_dataframe(df, text_column)
                logger.info("âœ… LLMæƒ…æ„Ÿåˆ†æå®Œæˆ")
            except Exception as e:
                logger.error(f"âŒ LLMåˆ†æå™¨åˆå§‹åŒ–æˆ–æ‰§è¡Œå¤±è´¥: {e}")
                logger.warning("âŒ å°†å›é€€åˆ°ä¼ ç»Ÿåˆ†æå™¨")
                use_traditional = True
        else:
            logger.warning("âŒ LLMåˆ†æå™¨ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿåˆ†æå™¨")
            use_traditional = True
    except Exception as e:
        logger.error(f"âŒ {model_type.upper()}æ¨¡å‹åˆ†æå¤±è´¥: {e}")
        # å³ä½¿LLMåˆ†æå¤±è´¥ï¼Œä¹Ÿå°è¯•ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•
        use_traditional = True
    
    # ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•è¿›è¡Œæƒ…æ„Ÿåˆ†æ
    if use_traditional:
        try:
            logger.info("ğŸ“ˆ ä½¿ç”¨ä¼ ç»Ÿæƒ…æ„Ÿåˆ†ææ–¹æ³•ä½œä¸ºè¡¥å……...")
            traditional_analyzer = TraditionalSentimentAnalyzer()
            analyzed_df = traditional_analyzer.analyze_dataframe(analyzed_df, text_column)
            logger.info("âœ… ä¼ ç»Ÿæƒ…æ„Ÿåˆ†æå®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ ä¼ ç»Ÿæƒ…æ„Ÿåˆ†æå¤±è´¥: {e}")
    
    return analyzed_df

def visualize_results(df: pd.DataFrame, save: bool = False) -> None:
    """å¯è§†åŒ–åˆ†æç»“æœ"""
    logger.info("å¼€å§‹ç”Ÿæˆå¯è§†åŒ–ç»“æœ")
    
    visualizer = SentimentVisualizer()
    
    # æå–æƒ…æ„Ÿåˆ†æç›¸å…³åˆ—
    sentiment_column = 'sentiment_sentiment' if 'sentiment_sentiment' in df.columns else 'textblob_sentiment'
    score_column = 'sentiment_score' if 'sentiment_score' in df.columns else 'textblob_polarity'
    
    # æå–æƒ…ç»ªåˆ—
    emotion_columns = [col for col in df.columns if col.startswith('emotion_score_')]
    
    # ç¡®ä¿æœ‰å¿…è¦çš„åˆ—
    if sentiment_column not in df.columns:
        logger.warning(f"æœªæ‰¾åˆ°æƒ…æ„Ÿåˆ—: {sentiment_column}")
        return
    
    # åˆ›å»ºç»¼åˆä»ªè¡¨ç›˜
    visualizer.create_summary_dashboard(
        df, 
        sentiment_column=sentiment_column,
        score_column=score_column,
        emotion_columns=emotion_columns if emotion_columns else [],
        text_column='text',
        save=save
    )
    
    logger.info("å¯è§†åŒ–å®Œæˆ")

def main():
    """ä¸»å‡½æ•°"""
    try:
        # æ¬¢è¿ä¿¡æ¯
        print("\n" + "="*60)
        print("ğŸ‰ ç¤¾äº¤åª’ä½“æƒ…æ„Ÿåˆ†æç³»ç»Ÿï¼ˆè¯¾å ‚æ¼”ç¤ºç‰ˆï¼‰")
        print("ğŸ“Š æ”¯æŒå®æ—¶æƒ…æ„Ÿå’Œæƒ…ç»ªåˆ†æ")
        print("ğŸ’» ä½¿ç”¨Hugging Faceå…è´¹æ¨¡å‹ï¼Œæ— éœ€APIå¯†é’¥")
        print("="*60 + "\n")
        
        # è§£æå‚æ•°
        args = parse_arguments()
        
        # åŠ è½½æ•°æ®
        if args.use_sample:
            logger.info("ğŸ”„ æ­£åœ¨åŠ è½½ç¤ºä¾‹æ•°æ®...")
            df = load_sample_data()
        elif args.data:
            logger.info(f"ğŸ“ æ­£åœ¨åŠ è½½æ•°æ®æ–‡ä»¶: {args.data}...")
            loader = DataLoader()
            df = loader.load_data(args.data)
        else:
            logger.error("âŒ è¯·æŒ‡å®šæ•°æ®æ–‡ä»¶æˆ–ä½¿ç”¨ --use-sample å‚æ•°")
            return
        
        logger.info(f"âœ… æˆåŠŸåŠ è½½æ•°æ®ï¼Œå…± {len(df)} æ¡è®°å½•")
        logger.info(f"ğŸ“ æ–‡æœ¬åˆ—: {args.text_column}")
        logger.info(f"ğŸ§  ä½¿ç”¨æ¨¡å‹: {args.model_type} - {args.model}")
        
        # æ•°æ®é¢„å¤„ç†
        if not args.skip_processing:
            logger.info("ğŸ§¹ æ­£åœ¨é¢„å¤„ç†æ•°æ®...")
            df = preprocess_data(df, args.text_column, args.language)
            text_column = f"{args.text_column}_cleaned"
            logger.info(f"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆï¼Œä½¿ç”¨æ¸…ç†åçš„æ–‡æœ¬åˆ—: {text_column}")
        else:
            text_column = args.text_column
            logger.info(f"â© è·³è¿‡é¢„å¤„ç†ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ–‡æœ¬åˆ—: {text_column}")
        
        # æ‰§è¡Œæƒ…æ„Ÿåˆ†æ - ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹ç±»å‹ï¼ˆé»˜è®¤Hugging Faceï¼‰
        analyzed_df = analyze_sentiment(
            df, 
            text_column,
            args.model,
            model_type=args.model_type,  # æ·»åŠ æ¨¡å‹ç±»å‹å‚æ•°
            use_traditional=args.use_traditional,
            batch_size=args.batch_size
        )
        
        # ä¿å­˜ç»“æœ - æ·»åŠ æ›´å‹å¥½çš„è¾“å‡ºä¿¡æ¯
        output_path = os.path.join(PROCESSED_DATA_DIR, args.output)
        analyzed_df.to_csv(output_path, index=False, encoding='utf-8')
        logger.info(f"ğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        
        # æ˜¾ç¤ºç®€è¦ç»Ÿè®¡ä¿¡æ¯ - é€‚åˆè¯¾å ‚å±•ç¤º
        logger.info(f"\nğŸ“Š åˆ†æç»“æœç»Ÿè®¡:")
        if 'sentiment_sentiment' in analyzed_df.columns:
            sentiment_counts = analyzed_df['sentiment_sentiment'].value_counts()
            logger.info(f"æƒ…æ„Ÿåˆ†å¸ƒ:")
            for sentiment, count in sentiment_counts.items():
                percentage = (count / len(analyzed_df)) * 100
                logger.info(f"  - {sentiment}: {count}æ¡ ({percentage:.1f}%)")
        
        if 'emotion_primary_emotion' in analyzed_df.columns:
            emotion_counts = analyzed_df['emotion_primary_emotion'].value_counts()
            logger.info(f"ä¸»è¦æƒ…ç»ªåˆ†å¸ƒ:")
            for emotion, count in emotion_counts.items():
                percentage = (count / len(analyzed_df)) * 100
                logger.info(f"  - {emotion}: {count}æ¡ ({percentage:.1f}%)")
        
        # å¯è§†åŒ–
        if args.visualize:
            logger.info("ğŸ¨ æ­£åœ¨ç”Ÿæˆå¯è§†åŒ–ç»“æœ...")
            visualize_results(analyzed_df, args.save_visualizations)
        
        logger.info("\nğŸ‰ æƒ…æ„Ÿåˆ†æä»»åŠ¡å®Œæˆï¼")
        logger.info("âœ… ç³»ç»Ÿä½¿ç”¨Hugging Faceå…è´¹æ¨¡å‹ï¼Œæ— éœ€APIå¯†é’¥å³å¯è¿è¡Œ")
        logger.info("ğŸ“± é€‚åˆè¯¾å ‚å®ç‰©å±•ç¤ºï¼Œæä¾›å®æ—¶åˆ†æå’Œå‹å¥½çš„å¯è§†åŒ–æ•ˆæœ")
        
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        raise

if __name__ == "__main__":
    main()